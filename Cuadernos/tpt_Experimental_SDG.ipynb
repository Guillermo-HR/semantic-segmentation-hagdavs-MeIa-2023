{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axRaOiI1-ZqR"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip uninstall tensorflow -y\n",
        "!pip uninstall tensorflow-io -y\n",
        "!pip install tensorflow\n",
        "!pip install --no-deps tensorflow-io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzewxQFMAt9X"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvTDZOyV-6OH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "import shutil\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from tensorflow.data import AUTOTUNE\n",
        "# Se importan las librerias que se utilizaran a lo largo de la ejecución del proyecto. Donde dentro de las relevantes para el ciclo de clasificación de imagenes se encuentran Tensor Flow, Sklearn y MatplotLib.\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#Encoding and Split data into Train/Test Sets\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#import scikitplot as skplt\n",
        "#Plot Images\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Cargar librerias necesariasnp\n",
        "import math\n",
        "from skimage import io, color\n",
        "import tensorflow\n",
        "import glob\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "import os\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mOdhmMiCmNK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O1xcFI8--ud"
      },
      "outputs": [],
      "source": [
        "data_dir = pathlib.Path('/content/drive/MyDrive/Colab Notebooks/RETO MEIA/HAGDAVS')\n",
        "images_dir = data_dir / 'RGB'\n",
        "masks_dir = data_dir / 'MASK'\n",
        "\n",
        "folder_path = masks_dir  # Ruta de la carpeta que contiene las imágenes\n",
        "new_folder_path = '/content/drive/MyDrive/Colab Notebooks/RETO MEIA/HAGDAVS/MASK'\n",
        "\n",
        "# Crear la nueva carpeta si no existe\n",
        "os.makedirs(new_folder_path, exist_ok=True)\n",
        "\n",
        "# Obtener la lista de nombres de archivo en la carpeta\n",
        "file_list = os.listdir(new_folder_path)\n",
        "\n",
        "# Aplicar la modificación del nombre y crear nuevos archivos en la nueva carpeta\n",
        "for filename in file_list:\n",
        "    if \"MClass\" in filename:\n",
        "        new_filename = filename.replace(\"MClass\", \"RGB\")\n",
        "        old_path = os.path.join(folder_path, filename)\n",
        "        new_path = os.path.join(new_folder_path, new_filename)\n",
        "        shutil.copy2(old_path, new_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et08ZZtxDBr7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import pathlib\n",
        "\n",
        "# Ruta a las carpetas de imágenes y máscaras\n",
        "ima_dir = pathlib.Path('/content/drive/MyDrive/Colab Notebooks/RETO MEIA/HAGDAVS')\n",
        "images_dir = ima_dir / 'RGB'\n",
        "m_dir = pathlib.Path('/content/drive/MyDrive/Colab Notebooks/RETO MEIA/HAGDAVS')\n",
        "mask_dir = m_dir / 'MASK'\n",
        "\n",
        "# Obtener una lista de rutas de archivo para imágenes y máscaras\n",
        "image_paths = sorted([str(path) for path in images_dir.glob('*.tif')])\n",
        "mask_paths = sorted([str(path) for path in mask_dir.glob('*.tif')])\n",
        "\n",
        "# Crear una lista para almacenar los parches de imágenes y máscaras\n",
        "image_patches = []\n",
        "mask_patches = []\n",
        "\n",
        "# Tamaño del parche deseado\n",
        "patch_size = (256, 256\n",
        "              )\n",
        "\n",
        "\n",
        "# Definir una función para cargar y dividir las imágenes y máscaras en parches\n",
        "def load_and_split_patches(image_path, mask_path):\n",
        "    image = tfio.experimental.image.decode_tiff(tf.io.read_file(image_path))\n",
        "    mask = tfio.experimental.image.decode_tiff(tf.io.read_file(mask_path))\n",
        "\n",
        "    # Eliminar el canal alfa de las imágenes y máscaras\n",
        "    image = image[:, :, :3]\n",
        "    mask = mask[:, :, :3]\n",
        "\n",
        "    # Dividir la imagen en parches\n",
        "    for i in range(0, image.shape[0], patch_size[0]):\n",
        "        for j in range(0, image.shape[1], patch_size[1]):\n",
        "            patch_image = image[i:i+patch_size[0], j:j+patch_size[1], :]\n",
        "            patch_mask = mask[i:i+patch_size[0], j:j+patch_size[1], :]\n",
        "\n",
        "            # Verificar si todos los píxeles en el parche de la máscara son negros\n",
        "            if tf.reduce_all(tf.math.equal(patch_mask, [0, 0, 0])):\n",
        "                continue\n",
        "\n",
        "            #patch_mask = convertir_colores(patch_mask)\n",
        "            image_patches.append(patch_image)\n",
        "            mask_patches.append(patch_mask)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Aplicar la función de carga y división de parches a cada par de rutas de archivo\n",
        "for image_path, mask_path in zip(image_paths, mask_paths):\n",
        "    load_and_split_patches(image_path, mask_path)\n",
        "\n",
        "# Convertir las listas de parches en tensores\n",
        "image_patches = tf.convert_to_tensor(image_patches)\n",
        "mask_patches = tf.convert_to_tensor(mask_patches)\n",
        "\n",
        "# Crear un dataset a partir de los parches de imágenes y máscaras\n",
        "dataset = tf.data.Dataset.from_tensor_slices((image_patches, mask_patches))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOcyI28QweAD"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2GHsLyQDOvn"
      },
      "outputs": [],
      "source": [
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = [\"Input Image\", \"True Mask\", \"Predicted Mask\"]\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
        "    plt.axis(\"off\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NclOtCkRDdYA"
      },
      "outputs": [],
      "source": [
        "def convertir_mascara(mascara):\n",
        "    mascara = tf.cast(mascara, dtype=tf.float32)\n",
        "    mascara_convertida = tf.zeros((256, 256, 1), dtype=tf.float32)\n",
        "\n",
        "    # Asigna valores correspondientes a cada clase\n",
        "    mascara_convertida = tf.where(tf.reduce_all(tf.equal(mascara, [0, 0, 0]), axis=-1, keepdims=True), 0.0, mascara_convertida)\n",
        "    mascara_convertida = tf.where(tf.reduce_all(tf.equal(mascara, [255, 0, 0]), axis=-1, keepdims=True), 0.0, mascara_convertida)\n",
        "    mascara_convertida = tf.where(tf.reduce_all(tf.equal(mascara, [0, 255, 0]), axis=-1, keepdims=True), 1.0, mascara_convertida)\n",
        "    mascara_convertida = tf.where(tf.reduce_all(tf.equal(mascara, [0, 0, 255]), axis=-1, keepdims=True), 0.0, mascara_convertida)\n",
        "\n",
        "    return mascara_convertida\n",
        "\n",
        "mapped_dataset = dataset.map(lambda x, y: (x, convertir_mascara(y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvloO6_GDikF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def filter_func(image, mask):\n",
        "    unique_classes = tf.unique(tf.reshape(mask, [-1]))[0]\n",
        "    return tf.size(unique_classes) >= 2\n",
        "\n",
        "filtered_dataset = mapped_dataset.filter(filter_func)\n",
        "\n",
        "dataset_length = 0\n",
        "for _ in filtered_dataset:\n",
        "    dataset_length += 1\n",
        "\n",
        "print(\"Longitud aproximada del dataset filtrado:\", dataset_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pWrMRpgwU7t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMPBEVZzE41T"
      },
      "outputs": [],
      "source": [
        "# dividir el dataset en conjuntos de entrenamiento, validación y prueba\n",
        "total_samples = (dataset_length)\n",
        "train_size = int(0.7 * total_samples)\n",
        "val_size = int(0.15 * total_samples)\n",
        "test_size = total_samples - train_size - val_size\n",
        "\n",
        "train_dataset = filtered_dataset.take(train_size)\n",
        "val_dataset = filtered_dataset.skip(train_size).take(val_size)\n",
        "test_dataset = filtered_dataset.skip(train_size + val_size).take(test_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__MRUbyzE-a0"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1iLcZz7FSiQ"
      },
      "outputs": [],
      "source": [
        "train_batches = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "train_batches = train_batches.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "validation_batches = test_dataset.batch(BATCH_SIZE)\n",
        "test_batches = test_dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7clGpg2FD2E"
      },
      "outputs": [],
      "source": [
        "def double_conv_block(x, n_filters):\n",
        "\n",
        "    # Conv2D then ReLU activation\n",
        "    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    # Conv2D then ReLU activation\n",
        "    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    # Conv2D then ReLU activation\n",
        "    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    # dropout\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def downsample_block(x, n_filters):\n",
        "    f = double_conv_block(x, n_filters)\n",
        "    p = layers.MaxPool2D(2)(f)\n",
        "    p = layers.Dropout(0.2)(p)\n",
        "\n",
        "    return f, p\n",
        "\n",
        "def upsample_block(x, conv_features, n_filters):\n",
        "    # upsample\n",
        "    x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
        "    # concatenate\n",
        "    x = layers.concatenate([x, conv_features])\n",
        "    # dropout\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    # Conv2D twice with ReLU activation\n",
        "    x = double_conv_block(x, n_filters)\n",
        "\n",
        "    return x\n",
        "\n",
        "def build_unet_model():\n",
        "\n",
        "    # inputs\n",
        "    inputs = layers.Input(shape=(256,256,3))\n",
        "\n",
        "    # encoder: contracting path - downsample\n",
        "    # 1 - downsample\n",
        "    f1, p1 = downsample_block(inputs, 64)\n",
        "    # 2 - downsample\n",
        "    f2, p2 = downsample_block(p1, 128)\n",
        "    # 3 - downsample\n",
        "    f3, p3 = downsample_block(p2, 256)\n",
        "    # 4 - downsample\n",
        "    f4, p4 = downsample_block(p3, 512)\n",
        "\n",
        "    # 5 - bottleneck\n",
        "    bottleneck = double_conv_block(p4, 1024)\n",
        "    bottleneck = layers.Dropout(0.3)(bottleneck)\n",
        "\n",
        "    # decoder: expanding path - upsample\n",
        "    # 6 - upsample\n",
        "    u6 = upsample_block(bottleneck, f4, 512)\n",
        "    # 7 - upsample\n",
        "    u7 = upsample_block(u6, f3, 256)\n",
        "    # 8 - upsample\n",
        "    u8 = upsample_block(u7, f2, 128)\n",
        "    # 9 - upsample\n",
        "    u9 = upsample_block(u8, f1, 64)\n",
        "\n",
        "    # outputs\n",
        "    outputs = layers.Conv2D(1, 1, padding=\"same\", activation = \"sigmoid\")(u9)\n",
        "\n",
        "    # unet model with Keras Functional API\n",
        "    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
        "\n",
        "    return unet_model\n",
        "\n",
        "unet_model = build_unet_model()\n",
        "\n",
        "#loss = keras.losses.sparse_categorical_crossentropy()\n",
        "\n",
        "unet_model.compile(optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=0.1),\n",
        "                   loss='binary_crossentropy',\n",
        "                   metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C27V34P2FD7S"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "\n",
        "STEPS_PER_EPOCH = total_samples // BATCH_SIZE\n",
        "\n",
        "VAL_SUBSPLITS = 5\n",
        "VALIDATION_STEPS = test_size // BATCH_SIZE // VAL_SUBSPLITS\n",
        "\n",
        "model_history = unet_model.fit(train_batches,\n",
        "                               epochs=NUM_EPOCHS,\n",
        "                               steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                               validation_steps=VALIDATION_STEPS,\n",
        "                               validation_data=validation_batches,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30b-2nbVjXcg"
      },
      "outputs": [],
      "source": [
        "def create_mask(pred_mask, threshold=0.2):\n",
        "  pred_mask = tf.cast(pred_mask, dtype=tf.float32)  # convertir a float\n",
        "  pred_mask = tf.where(pred_mask > threshold, 1.0, 0.0)  # usar float para el threshold\n",
        "  return pred_mask[0]\n",
        "\n",
        "def show_predictions(dataset, model, num):\n",
        "  if dataset:\n",
        "    for image, mask in dataset.take(num):\n",
        "      pred_mask = model.predict(image)\n",
        "      display([image[0], mask[0], create_mask(pred_mask)])\n",
        "  else:\n",
        "    display([sample_image, sample_mask,\n",
        "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3TG-hEBjaiT"
      },
      "outputs": [],
      "source": [
        "show_predictions(test_batches, unet_model, 16)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
